{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only has the train split, so we divide it overselves\n",
    "ds_raw = load_dataset(\"opus_books\", \"en-it\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '4', 'translation': {'en': 'There was no possibility of taking a walk that day.', 'it': 'I. In quel giorno era impossibile passeggiare.'}}\n"
     ]
    }
   ],
   "source": [
    "print(ds_raw[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer_src = Tokenizer.from_file(str(r\"D:\\Machine Learning\\Transformers\\Transformer_From_Scratch\\pytorch-transformer\\tokenizer_en.json\"))\n",
    "tokenizer_tgt = Tokenizer.from_file(str(r\"D:\\Machine Learning\\Transformers\\Transformer_From_Scratch\\pytorch-transformer\\tokenizer_it.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2   2\n",
      "3   3\n",
      "1   1\n",
      "1   None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_src.token_to_id(\"[SOS]\"),\" \",tokenizer_tgt.token_to_id(\"[SOS]\"))\n",
    "print(tokenizer_src.token_to_id(\"[EOS]\"),\" \",tokenizer_tgt.token_to_id(\"[EOS]\"))\n",
    "print(tokenizer_src.token_to_id(\"[PAD]\"),\" \",tokenizer_tgt.token_to_id(\"[PAD]\"))\n",
    "print(tokenizer_src.token_to_id(\"[PAD]\"),\" \",tokenizer_tgt.token_to_id(\"Arunim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[237, 14, 67, 1693, 10, 442, 11, 703, 15, 132, 7]\n",
      "['There', 'was', 'no', 'possibility', 'of', 'taking', 'a', 'walk', 'that', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_src.encode('There was no possibility of taking a walk that day.').ids)\n",
    "print(tokenizer_src.encode('There was no possibility of taking a walk that day.').tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '.', 'In', 'quel', 'giorno', 'era', 'impossibile', 'passeggiare', '.']\n",
      "[330, 5, 208, 76, 147, 22, 619, 3860, 5]\n",
      "['I', '.', 'In', '[UNK]', '[UNK]', 'era', '[UNK]', '[UNK]', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_tgt.encode('I. In quel giorno era impossibile passeggiare.').tokens)\n",
    "print(tokenizer_tgt.encode('I. In quel giorno era impossibile passeggiare.').ids)\n",
    "print(tokenizer_src.encode('I. In quel giorno era impossibile passeggiare.').tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])   tensor([3])   tensor([1])\n",
      "337\n",
      "tensor([   2,  237,   14,   67, 1693,   10,  442,   11,  703,   15,  132,    7,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "enc_input_tokens = tokenizer_src.encode('There was no possibility of taking a walk that day.').ids\n",
    "sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")],dtype=torch.int64)\n",
    "eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")],dtype=torch.int64)\n",
    "pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")],dtype=torch.int64)\n",
    "print(sos_token,\" \",eos_token,\" \",pad_token)\n",
    "enc_num_padding_tokens = 350 - len(enc_input_tokens) - 2\n",
    "print(enc_num_padding_tokens)\n",
    "encoder_input = torch.cat(\n",
    "    [   sos_token,\n",
    "        torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "        eos_token,\n",
    "        torch.tensor([pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1,2,32)\n",
    "print(x[:,-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 39, 41, 41, 41, 50, 50, 143, 386, 710, 900]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([ 41, 50, 143, 386, 23, 41, 50, 710, 39, 41, 900 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71 0.3  0.85 0.85 0.31 0.24 0.39 0.15 0.99 0.36]\n",
      "[0.58 0.12 0.69 0.72 0.23 0.92 0.02 0.65 0.91 0.25]\n",
      "[0.85 0.29 0.73 0.78 0.07 0.34 0.86 0.37 0.3  0.33]\n",
      "[0.9  0.76 0.69 0.98 0.4  0.08 0.16 0.75 0.75 0.71]\n",
      "[0.61 0.42 0.62 0.9  0.29 0.25 0.06 0.58 0.25 0.55]\n",
      "[0.91 0.57 0.36 0.29 0.67 0.49 1.   0.25 0.54 0.06]\n",
      "[0.7  0.83 0.77 0.25 0.89 0.98 0.16 0.66 0.57 0.16]\n",
      "[0.39 0.6  0.91 0.95 0.25 0.27 0.07 0.24 0.68 0.26]\n",
      "[0.6  0.65 0.2  0.41 0.08 0.11 0.09 0.03 0.24 0.88]\n",
      "[0.36 0.31 0.51 0.69 0.92 0.19 0.83 0.66 0.18 0.77]\n",
      "[0.76 0.08 0.34 0.72 0.6  0.72 0.94 0.93 0.46 0.41]\n",
      "[0.91 0.57 0.62 0.7  0.27 0.1  0.33 0.43 0.1  0.86]\n",
      "[0.25 0.62 0.55 0.68 0.92 0.12 0.68 0.37 0.42 0.58]\n",
      "[0.75 0.42 0.56 0.66 0.08 0.42 0.69 0.87 0.32 0.69]\n",
      "[0.1  0.65 0.31 0.1  0.34 0.31 0.76 0.37 0.32 0.9 ]\n",
      "[0.8  0.64 0.71 0.65 0.56 0.43 0.24 0.88 0.81 0.23]\n",
      "[0.98 0.87 0.63 0.28 0.12 0.2  0.8  0.74 0.   0.82]\n",
      "[0.17 0.06 0.8  0.21 0.66 0.8  0.   0.28 0.88 1.  ]\n",
      "[0.65 0.55 0.61 0.07 0.28 0.   0.21 0.57 0.44 0.88]\n",
      "[0.25 0.68 0.2  0.97 0.92 0.19 0.4  0.76 0.22 0.82]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(20,10)\n",
    "for i in range(20):\n",
    "    print(x[i].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Transformers\\transformers\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The sky turned a brilliant shade of orange as the sun set over the horizon.\",\n",
    "    \"As the sun dipped below the horizon, the sky was painted in vibrant hues of orange.\"\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences,normalize_embeddings=None)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8696]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8695888"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "print(util.cos_sim(embeddings[0],embeddings[1]))\n",
    "np.dot(embeddings[0],embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
